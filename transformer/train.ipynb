{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4801268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from decoder import subsequent_mask\n",
    "from Transformer import make_model\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ea74a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    def __init__(self, src, trg=None, pad=0):\n",
    "        \"\"\"\n",
    "        定义一个批处理对象，其中包含用于训练的源句子和目标句子，以及构建掩码。\n",
    "        :param src: (Tensor)\n",
    "        :param trg: (Tensor)\n",
    "        :param pad:\n",
    "        \"\"\"\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:, :-1]\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            self.trg_mask = self.make_std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "\n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"\"\"\n",
    "        生成一个mask来隐藏填充将来出现的词\n",
    "        :param tgt:\n",
    "        :param pad:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 在倒数第二个维度上增加一个维度\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e342049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(V, batch, nbatches):\n",
    "    \"\"\"\n",
    "    为src-tgt copy task随机生成数据.\n",
    "    :param V: (int)生成数据的最大值\n",
    "    :param batch: (int) 批尺寸\n",
    "    :param nbatches: (int) 批数\n",
    "    :return: 一个 iterable 对象，\n",
    "    \"\"\"\n",
    "    for i in range(nbatches):\n",
    "        # 生成[0-V）的随机整数，尺寸为[batch,10]\n",
    "        data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10)))\n",
    "        # 第一列都置为1\n",
    "        data[:, 0] = 1\n",
    "        src = Variable(data, requires_grad=False)\n",
    "        tgt = Variable(data, requires_grad=False)\n",
    "        # yield 的作用就是把一个函数变成一个 generator，带有 yield 的函数不再是一个普通函数，而是返回一个 iterable 对象\n",
    "        yield Batch(src, tgt, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8a6b024",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        \"\"\"\n",
    "        计算损失的类\n",
    "        :param generator: 模型的生成器，即transformer最后的预测输出层，对应linea+softmax\n",
    "        :param criterion: 标签平滑的惩罚项\n",
    "        :param opt: 优化器\n",
    "        \"\"\"\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)) / norm\n",
    "\n",
    "        loss.backward()\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        # return loss.data.[0] * norm\n",
    "        return loss.data.item() * norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9f2a5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    \"\"\"\n",
    "    贪心解码:取解码器输出概率最高的那个词\n",
    "    :param model:\n",
    "    :param src:\n",
    "    :param src_mask:\n",
    "    :param max_len:\n",
    "    :param start_symbol:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # memory保存的是编码器输出结果\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len - 1):\n",
    "        out = model.decode(memory, src_mask,\n",
    "                           Variable(ys),\n",
    "                           Variable(subsequent_mask(ys.size(1)).type_as(src.data)))\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7c5d683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gedun/anaconda3/envs/nlp_test/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/Users/gedun/找工作_练习/transformer_test/Transformer.py:83: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(p)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Step: 1 Loss: 3.376153 Tokens per Sec: 1884.705566\n",
      "Epoch Step: 1 Loss: 1.907213 Tokens per Sec: 3436.514404\n",
      "save the best model successful!\n",
      "tensor(1.9390)\n",
      "Epoch Step: 1 Loss: 2.046899 Tokens per Sec: 1741.895630\n",
      "Epoch Step: 1 Loss: 1.665348 Tokens per Sec: 3441.882812\n",
      "save the best model successful!\n",
      "tensor(1.6398)\n",
      "Epoch Step: 1 Loss: 1.911526 Tokens per Sec: 2454.457275\n",
      "Epoch Step: 1 Loss: 1.479995 Tokens per Sec: 3515.309082\n",
      "save the best model successful!\n",
      "tensor(1.5001)\n",
      "Epoch Step: 1 Loss: 1.780898 Tokens per Sec: 2384.590088\n",
      "Epoch Step: 1 Loss: 1.352124 Tokens per Sec: 3466.717041\n",
      "save the best model successful!\n",
      "tensor(1.3514)\n",
      "Epoch Step: 1 Loss: 1.326282 Tokens per Sec: 2411.867188\n",
      "Epoch Step: 1 Loss: 1.011599 Tokens per Sec: 3316.849854\n",
      "save the best model successful!\n",
      "tensor(0.9905)\n",
      "Epoch Step: 1 Loss: 1.390555 Tokens per Sec: 2418.183838\n",
      "Epoch Step: 1 Loss: 0.786327 Tokens per Sec: 3425.791260\n",
      "save the best model successful!\n",
      "tensor(0.6936)\n",
      "Epoch Step: 1 Loss: 0.890221 Tokens per Sec: 2361.635498\n",
      "Epoch Step: 1 Loss: 0.484875 Tokens per Sec: 3099.722900\n",
      "save the best model successful!\n",
      "tensor(0.4424)\n",
      "Epoch Step: 1 Loss: 0.757809 Tokens per Sec: 2246.282227\n",
      "Epoch Step: 1 Loss: 0.260387 Tokens per Sec: 3431.469238\n",
      "save the best model successful!\n",
      "tensor(0.3065)\n",
      "Epoch Step: 1 Loss: 0.275000 Tokens per Sec: 2231.791016\n",
      "Epoch Step: 1 Loss: 0.292636 Tokens per Sec: 3290.314453\n",
      "save the best model successful!\n",
      "tensor(0.3206)\n",
      "Epoch Step: 1 Loss: 0.488880 Tokens per Sec: 1811.455322\n",
      "Epoch Step: 1 Loss: 0.384353 Tokens per Sec: 3208.516846\n",
      "save the best model successful!\n",
      "tensor(0.3394)\n",
      "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 训练src-tgt copy task\n",
    "    V = 11\n",
    "    criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
    "    model = make_model(V, V, N=2)\n",
    "    model_opt = NoamOpt(model.src_embed[0].d_model, 1, 400,\n",
    "                        torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "\n",
    "    losslast=float('inf')\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        run_epoch(data_gen(V, 30, 20), model, SimpleLossCompute(model.generator, criterion, model_opt))\n",
    "        # 验证，不需要优化器\n",
    "        model.eval()\n",
    "        loss = run_epoch(data_gen(V, 30, 5), model, SimpleLossCompute(model.generator, criterion, None))\n",
    "        if loss < losslast:\n",
    "            if not os.path.exists(\"model\"):\n",
    "                os.mkdir(\"model\")\n",
    "            torch.save(model.state_dict(), 'model\\copytask.pkl')\n",
    "            print(\"save the best model successful!\")\n",
    "        print(loss)\n",
    "\n",
    "    model.eval()\n",
    "    src = Variable(torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]))\n",
    "    src_mask = Variable(torch.ones(1, 1, 10))\n",
    "    print(greedy_decode(model, src, src_mask, max_len=10, start_symbol=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77459753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e826544e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d41b48f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e4f6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bfbfb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd97aa3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaaacc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a025a2a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff71ff7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fd8c94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_test",
   "language": "python",
   "name": "nlp_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
