{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4235f66b",
   "metadata": {},
   "source": [
    "# 整体调度处理模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1137ac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from attention import MultiHeadedAttention\n",
    "from positionwiseFeedForward import PositionwiseFeedForward\n",
    "from positional_encoding import PositionalEncoding\n",
    "from encoder import Encoder, EncoderLayer\n",
    "from decoder import Decoder, DecoderLayer\n",
    "from embedding_softmax import Embeddings\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a32af4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, input_embed, target_embed, line_softmax):\n",
    "        \"\"\"\n",
    "        Encoder - Decoder架构\n",
    "        1 encoder：编码器模型(nn.Module)\n",
    "        2 decoder：解码器模型(nn.Module)\n",
    "        3 input_embed: embedding后的encoder测的输入数据(nn.Module)\n",
    "        4 target_embed: embedding后decodeer侧的输入向量(也就是目标向量)(nn.Module)\n",
    "        5 line_softmax: 模型decoder侧最后的linear --> softmax(nn.Module)\n",
    "        \"\"\"\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.input_embed = input_embed\n",
    "        self.target_embed = target_embed\n",
    "        self.line_softmax = line_softmax\n",
    "    \n",
    "    def forward(self, input_tensor, target_tensor, input_mask, target_mask):\n",
    "        \"\"\"\n",
    "        input_tensor:输入数据的向量结果\n",
    "        input_mask：输入数据对应的mask掩码\n",
    "        target同理\n",
    "        \"\"\"\n",
    "        return self.decode(self.encode(input_tensor, input_mask), input_mask, target_tensor, target_mask)\n",
    "    \n",
    "    def encode(self, input_tensor, input_mask):\n",
    "        return self.encoder(self.input_embed(input_tensor), input_mask)\n",
    "    \n",
    "    def decode(self, memory, input_mask, target_tensor, target_mask):\n",
    "        return self.decoder(self.target_embed(target_tensor), memory, input_mask, target_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bc9f234",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineSoftmax(nn.Module):\n",
    "    \"\"\"定义标准的linear --> softmax.\"\"\"\n",
    "    def __init__(self, embed_dim, vocab_size):\n",
    "        \"\"\"\n",
    "        embed_dim: 词向量embedding后的维度\n",
    "        input_size： 词向量的尺寸\n",
    "        \"\"\"\n",
    "        super(LineSoftmax, self).__init__()\n",
    "        self.line = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.line(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c1982d",
   "metadata": {},
   "source": [
    "# 构建整体模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d958e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_vocab_size, target_vocab_size, N=6, embed_dim=512, encode_dim=2048, h=8, dropout=0.1):\n",
    "    \"\"\"\n",
    "    input_vocab_size:输入词向量的尺寸\n",
    "    N：编码解码层的重复次数(N个Encoder/Decoder block)\n",
    "    encode_dim:编码器内层维度\n",
    "    h： 'Scaled Dot Product Attention'，使用的次数\n",
    "    dropout： 丢弃机制\n",
    "    \"\"\"\n",
    "    c = copy.deepcopy\n",
    "    # 实例化多头，前馈层，位置编码\n",
    "    attention = MultiHeadedAttention(h, embed_dim)\n",
    "    ff = PositionwiseFeedForward(embed_dim, dropout)\n",
    "    position = PositionalEncoding(embed_dim, dropout)\n",
    "    \n",
    "    \"\"\"\n",
    "    根据结构图, 最外层是EncoderDecoder，在EncoderDecoder中，\n",
    "    分别是编码器层，解码器层，源数据Embedding层和位置编码组成的有序结构，\n",
    "    目标数据Embedding层和位置编码组成的有序结构，以及类别生成器层.\n",
    "    在编码器层中有attention子层以及前馈全连接子层，\n",
    "    在解码器层中有两个attention子层以及前馈全连接层.\n",
    "    \"\"\"\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(embed_dim, c(attention), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(embed_dim, c(attention), c(attention), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(embed_dim, input_vocab_size), c(position)),\n",
    "        nn.Sequential(Embeddings(embed_dim, target_vocab_size), c(position)),\n",
    "        LineSoftmax(embed_dim, target_vocab_size)\n",
    "    )\n",
    "    \n",
    "    # 模型权重初始化使用xavier_uniform\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b75963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7558c53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbc87a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a55382d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b5df6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe8dc4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7daa9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edede46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d15cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ae7015",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_test",
   "language": "python",
   "name": "nlp_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
